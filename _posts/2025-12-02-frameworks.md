---
layout: post
title: "AI Global Frameworks, Implementation Gaps"
date: 2025-02-11
excerpt: A comparative synthesis of major AI governance frameworks and 2023–2025 evidence showing persistent implementation gaps, misalignment with public concerns, and design principles to strengthen accountability, capacity, and trust.
---

## Governing Intelligent Systems: Global Frameworks, Implementation Gaps, and Pathways to Protect Humanity without stiffling Innovation

**Munongedzi Mabhoko, Clarkson University**  
Email: [mabhokm@clarkson.edu](mailto:mabhokm@clarkson.edu)

## Abstract

Over the last decade, governments, standards bodies, and firms have converged on a shared vocabulary of “trustworthy,” “responsible,” and “human-centric” AI. Major frameworks now exist at global (UNESCO, OECD, UN, G7), regional (EU AI Act, African Union Continental AI Strategy, China’s algorithmic measures), national (US executive actions, sectoral rules), and organizational levels (NIST AI Risk Management Framework, ISO/IEC 42001).

Yet empirical evidence from 2023–2025 shows a persistent implementation gap: AI and other intelligent systems diffuse faster than governance structures, and real-world practices frequently fall outside or beneath these frameworks. Surveys report that AI use is now mainstream (≈78% of organizations), while fewer than half of firms have any formal AI governance policy, and only a small minority have deeply implemented controls. Public trust is fragile. Global surveys show rising concern about fairness, privacy, and misinformation, especially in advanced economies.

This thesis synthesizes recent research on what has been built, how it has been implemented across regions (Europe, North America, China, Africa, emerging economies), why gaps persist, how “rogue elements” emerge (shadow AI, disinformation, unregulated uses), and what design changes are needed if governance is to protect human dignity while preserving the benefits of intelligent systems.

---

## Chapter 1: Introduction

### 1.1 Background

Intelligent systems, including contemporary machine learning models, decision-support algorithms, recommender systems, and large-scale generative models, now mediate access to work, credit, healthcare, education, media, and public services. Their rapid diffusion has outpaced traditional regulatory cycles, prompting a wave of new AI governance instruments at global, regional, and national levels.

These include:

- OECD AI Principles and implementation reviews  
- UNESCO Recommendation on the Ethics of Artificial Intelligence  
- European Union Artificial Intelligence Act (EU AI Act)  
- NIST AI Risk Management Framework (AI RMF) and Generative AI profile  
- China’s layered algorithm and generative AI regulations  
- African Union Continental AI Strategy  

At the same time, evidence shows adoption is mainstream while governance remains partial and uneven. Stanford’s AI Index reports dramatic growth in AI use among large firms, but most organizations have operationalized only a subset of risk mitigations they consider relevant. Public opinion surveys show that many publics are more concerned than excited about AI, with fears clustered around privacy, job displacement, manipulation, and loss of human agency. Shadow AI, the use of unapproved tools outside governance structures, has emerged as a widespread organizational reality.

This juxtaposition suggests a core problem: the presence of frameworks does not guarantee effective, trusted governance.

### 1.2 Research problem and questions

This thesis addresses a merged research question:

How effective are current AI governance frameworks (EU AI Act, NIST AI RMF, OECD Principles, UNESCO Recommendations, China’s AI regulations, AU strategy) at addressing real-world risks posed by intelligent systems, and to what extent do these frameworks align with public perceptions, fears, and expectations about AI? Additionally, how does this alignment or misalignment influence effectiveness and public trust?

This question has three dimensions:

1. **Effectiveness:** Do frameworks materially reduce real-world risks (discrimination, opacity, security failures, disinformation, harmful automation)?  
2. **Alignment:** Do framework priorities correspond to what publics fear, expect, and experience?  
3. **Interaction:** How does alignment or misalignment shape compliance, shadow AI, and trust?

### 1.3 Hypothesis

Current AI governance frameworks are structurally insufficient because they rely heavily on broad principles, voluntary compliance, and reactive policymaking. While they aim to mitigate risks like bias, opacity, disinformation, and algorithmic harm, they often fail to address specific concerns that the public experiences or fears. Misalignment between governance priorities and public sentiment weakens trust and reduces compliance, diminishing real-world effectiveness. A more effective model integrates enforceable mechanisms with participatory, human-centered design grounded in evidence and community input.

### 1.4 Significance and contributions

1. **Synthesis of frameworks:** Integrated cross-regional analysis of major governance instruments (EU, US/NIST, OECD, UNESCO, China, AU), covering scope, mechanisms, and implementation status.  
2. **Effectiveness assessment:** Evaluation using secondary evidence from surveys, sectoral case studies (especially healthcare), and comparative governance analyses.  
3. **Alignment analysis:** A conceptual lens for governance–public alignment showing how misalignment contributes to shadow AI, partial compliance, and contested legitimacy.

### 1.5 Thesis structure

- Chapter 2 reviews literature on governance frameworks, implementation, and public perceptions.  
- Chapter 3 outlines methodology: comparative policy analysis using secondary evidence and an alignment framework.  
- Chapter 4 presents findings on effectiveness and governance–public alignment across regions.  
- Chapter 5 discusses implications, revisits the hypothesis, and proposes design principles.  
- A brief conclusion summarizes contributions and future research.

---

## Chapter 2: Literature Review

### 2.1 Global and regional AI governance frameworks

#### 2.1.1 OECD AI Principles

The OECD AI Principles (2019) articulate values-based norms for “trustworthy AI.” A 2023 implementation review reports over 1,000 policy initiatives across 70 jurisdictions aligned with these principles. The principles emphasize inclusive growth, human-centered values, transparency, robustness, security, and accountability, but remain non-binding. Their effectiveness depends on translation into law, institutions, and practice.

#### 2.1.2 UNESCO Recommendation on the Ethics of AI

UNESCO’s 2021 Recommendation is the first global standard on AI ethics adopted by all member states. It anchors AI governance in human rights, dignity, and sustainability and calls for oversight, fairness, and transparency. It is normatively rich but lacks direct enforcement; impact depends on domestic uptake and funding.

#### 2.1.3 The EU AI Act

The EU AI Act entered into force in 2024 and is the most comprehensive binding AI law to date. It classifies systems into prohibited, high-risk, limited-risk, and minimal-risk categories, with obligations on risk management, data quality, transparency, human oversight, robustness, and cybersecurity, including provisions for GPAI and foundation models. Implementation is phased, with key requirements landing between 2025 and 2027.

#### 2.1.4 NIST AI Risk Management Framework

The NIST AI RMF (2023) is a voluntary framework organized around four functions: **Govern, Map, Measure, Manage**. In 2024 NIST released a Generative AI profile (AI 600-1) to address risks like hallucinations, data leakage, and manipulation. The RMF is increasingly referenced in federal guidance and private-sector compliance, but voluntary status limits reach.

#### 2.1.5 China’s AI regulatory regime

China has binding regulations for recommendation algorithms (2021), deep synthesis (2022), and generative AI (2023). These instruments combine information control with economic and social governance objectives, enforced through filing, audits, and platform accountability. China has moved quickly on rule-making but provides weaker transparency and rights safeguards.

#### 2.1.6 African Union Continental AI Strategy

The AU strategy (2024) emphasizes an Africa-centric, development-focused approach with pillars on benefits, capacity, risk minimization, investment, and cooperation. Implementation depends on national legal systems and capacity-building amid constraints such as infrastructure gaps and dependence on foreign platforms.

### 2.2 Conceptualizations of AI governance and responsible AI

Scoping and systematic reviews show expanding but fragmented governance literature. Key challenges include unclear responsibilities, lack of enforcement mechanisms, and limited empirical evaluation. Governance is increasingly framed as a multi-actor, multi-level problem shaped by geopolitical competition and regulatory fragmentation.

### 2.3 Implementation and effectiveness

Evidence documents substantial gaps between governance design and operationalization.

- Governmental implementation varies widely, with limited monitoring and auditing capacity in most jurisdictions.  
- Organizational surveys show risk awareness but weak embedded practice; fully implemented governance remains rare.  
- Sectoral case studies, especially in healthcare, show barriers including regulatory complexity, fragmented data, low AI literacy, interoperability issues, and equity risks.

### 2.4 Public perceptions, trust, and legitimacy

Public sentiment is ambivalent and context-dependent. Pew finds Americans are more concerned than excited about AI, citing job loss, surveillance, and fairness. Global surveys show regular use and expected benefits alongside perceptions that AI is untrustworthy and poorly regulated. Legitimacy hinges on fairness, transparency, and agency, not only outcomes.

### 2.5 Shadow AI and rogue practices

Shadow AI refers to unapproved AI use outside governance controls. Surveys show high workplace prevalence and frequent sharing of sensitive data, indicating an implementation failure: governance misaligned with user needs drives practice underground, weakening risk management and trust.

### 2.6 Research gap

Less work systematically compares major frameworks across regions, evaluates implementation effectiveness using empirical indicators, and explicitly analyzes alignment with public fears and expectations. This thesis addresses that gap by integrating policy analysis, implementation evidence, and public-perception data.

---

## Chapter 3: Methodology

### 3.1 Research design

A comparative mixed-methods synthesis combining:

1. Document and policy analysis of major frameworks  
2. Secondary empirical analysis of surveys and reports on adoption, implementation, and public perceptions  
3. An alignment framework assessing overlap or divergence between governance priorities and public concerns

### 3.2 Framework selection and scope

Frameworks selected are widely influential, binding or high-weight soft-law, and active between 2022–2025. Core set: OECD, UNESCO, UN advisory report, EU AI Act, AU strategy, China’s measures, and the US approach represented via NIST RMF and federal guidance.

### 3.3 Data sources

Four categories: implementation surveys, sectoral case studies, public trust surveys, and shadow AI evidence.

### 3.4 Analytical framework

Two stages:

#### 3.4.1 Evaluating effectiveness

Effectiveness assessed via:

- risk coverage  
- regulatory strength  
- institutional capacity  
- observed implementation  

Produces qualitative judgments (high, moderate, low) across framework-region pairs.

#### 3.4.2 Assessing governance–public alignment

Alignment defined by how well governance risk priorities and remedies match public concerns and legitimacy expectations. Assessed through framework content analysis, public opinion data, and case evidence of acceptance or backlash.

### 3.5 Limitations

Relies on secondary data, uses qualitative comparison rather than a quantitative scorecard, has limited global generalizability, and is temporally bounded in a rapidly evolving landscape.

---

## Chapter 4: Results and Analysis

### 4.1 Effectiveness of major frameworks

#### 4.1.1 OECD and UNESCO

High normative coverage but non-binding, with uneven domestic translation. Observed evidence suggests limited direct risk reduction to date.

**Assessment:** high normative value, low direct effectiveness without concrete instantiation.

#### 4.1.2 EU AI Act

Comprehensive risk coverage and binding obligations backed by strong penalties. Effectiveness depends on standards, regulator capacity, and implementation clarity.

**Assessment:** high potential effectiveness, contingent on execution.

#### 4.1.3 NIST RMF and US patchwork

Broad risk coverage and practical organizational processes, but voluntary. Federal references increase influence, yet enforcement remains fragmented and maturity uneven.

**Assessment:** flexible but uneven governance, strong for leaders, weaker system-wide.

#### 4.1.4 China

Binding rules with strong enforcement, particularly where political priorities apply. Effective at consolidating control and compelling provider obligations, but weaker on rights safeguards.

**Assessment:** effective in state-control terms, less aligned with human-rights-based governance.

#### 4.1.5 African Union Strategy

High relevance and development-centered framing, but low implementation capacity due to resource and infrastructure constraints.

**Assessment:** strong normative direction, constrained operationalization.

### 4.2 Cross-cutting patterns

1. **Proliferation of rules vs lagging implementation:** regulations increase alongside incidents.  
2. **Voluntary vs binding regimes:** soft-law spreads but lacks authority; binding rules enforce but face resistance and capacity constraints.  
3. **Sectoral imbalance:** high-stakes domains often lag in governance maturity.

### 4.3 Governance–public alignment

Framework priorities include fairness, transparency, safety, privacy, accountability. Public concerns extend to job displacement, surveillance, manipulation, and lack of participation. Many frameworks under-address economic precarity and political persuasion harms.

#### Shadow AI as misalignment symptom

Shadow AI grows when governance does not fit real workflows, driving risky practices underground.

#### Democratic legitimacy and persuasive AI

Persuasion risks of general-purpose models are only partially addressed. Public concern about misinformation and manipulation is not matched by strong governance mechanisms.

### 4.4 Consequences

Misalignment produces:

- erosion of trust  
- partial compliance  
- shadow and rogue practices  
- inequitable protection across regions  

These findings support the hypothesis that frameworks are structurally insufficient due to voluntary adoption, high-level principles, and weak integration of public experience and capacity realities.

---

## Chapter 5: Discussion and Recommendations

### 5.1 Interpreting findings

Evidence broadly supports the hypothesis, with nuance: binding regimes (EU, China) show stronger potential, the AU strategy foregrounds equity even if capacity lags. The core issue is that frameworks are not yet designed or implemented as socio-technical systems reflecting how people actually use and contest AI.

### 5.2 Design principles for effective, public-aligned governance

#### 5.2.1 From “AI” to intelligent systems

Governance should apply by function, autonomy, and impact—not technical labels—closing loopholes around algorithmic scoring, monitoring, and surveillance.

#### 5.2.2 Harden soft-law with enforceable mechanisms

Pair soft-law with binding requirements for high-risk domains, third-party audits/certifications, and incident reporting obligations.

#### 5.2.3 Institutionalize participation and co-design

Require consultations and create citizen assemblies for sensitive use cases. Embed worker councils and ethics boards at organizational level.

#### 5.2.4 Build capacity and shared infrastructure in the Global South

Fund evaluation labs, testbeds, open-source auditing tools, and capacity-building for regulatory craft and community engagement.

#### 5.2.5 Design for shadow AI

Conduct AI use audits, provide safe sandboxes, and communicate governance as enabling safe use rather than only restricting tools.

#### 5.2.6 Address epistemic and political harms explicitly

Create standards for political content provenance and safeguards against automated persuasion, and support public-interest research.

#### 5.2.7 Embed accountability via personal and corporate liability

Shift from system-only accountability to consequences for developers and decision-makers:

- personal liability for reckless deployment  
- fines proportionate to global revenue  
- mandatory disclosure of high-risk models  
- authority to ban repeat offenders from sectors  

These mechanisms reduce moral hazard by attaching consequences to foreseeable harms.

### 5.3 Implications

For policymakers:

- invest in implementation capacity, not just rule drafting  
- use hybrid models (binding + codes + soft-law)  
- treat trust metrics as policy indicators  

For organizations:

- make governance core to risk and strategy  
- use RMF/ISO for continuous improvement and transparency  
- engage workers and users in AI-enabled workflow design  

For researchers:

- empirical evaluation of governance effectiveness  
- comparative studies of perceptions across regimes  
- metrics for governance–public alignment beyond trust scores  

---

## Conclusion

Global and regional AI governance frameworks provide essential foundations, but remain only partially effective without mechanisms that address implementation gaps, public concerns, and emerging risks. The evidence shows persistent loopholes, uneven adoption, and misalignment between governance priorities and lived public concerns, contributing to shallow compliance, shadow AI, and declining trust.

Strengthening governance does not require abandoning existing frameworks. It requires building more vigilant, accountable, and empirically grounded layers on top of them: enforceable accountability, regulatory capacity, participatory oversight, and clearer liability for harmful development and deployment. By closing the gaps that allow intelligent systems to outpace protections, policymakers can create a governance ecosystem that better restores trust and advances societal well-being.

---

## References

1. Papagiannidis, E., Marikyan, D., Rana, N. P. (2024). Responsible artificial intelligence governance: A review. *Technological Forecasting and Social Change, 198*, 122840. https://doi.org/10.1016/j.jsis.2024.101885  
2. Batool, A., Alam, M., Ahmed, S. (2025). AI governance: A systematic literature review. *AI and Ethics, 5*(1), 33–58. https://doi.org/10.1007/s43681-024-00653-w  
3. Zaidan, E. (2024). AI governance in a complex and rapidly changing world. *Humanities and Social Sciences Communications, 11*, 105. https://doi.org/10.1057/s41599-024-03560-x  
4. OECD. (2023). *The State of Implementation of the OECD AI Principles Four Years On.* OECD AI Papers No. 3. https://doi.org/10.1787/835641c9-en  
5. UNESCO. (2021). *Recommendation on the Ethics of Artificial Intelligence.* https://unesdoc.unesco.org/ark:/48223/pf0000381137  
6. OECD. (2023). *OECD Framework for the Implementation of Trustworthy AI.* https://oecd.ai/en/ai-principles  
7. European Commission. (2024). *Artificial Intelligence Act (EU AI Act).* Official Journal of the European Union. https://artificialintelligenceact.eu/the-act  
8. NIST. (2023). *Artificial Intelligence Risk Management Framework (AI RMF 1.0).* https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf  
9. African Union Commission. (2024). *Continental Strategy for Artificial Intelligence.* https://au.int/sites/default/files/documents/44004-doc-EN-_Continental_AI_Strategy_July_2024.pdf  
10. Cyberspace Administration of China (CAC). (2021–2023). Algorithm Recommendation Provisions; Deep Synthesis Provisions; Interim Measures for Generative AI Services.  
11. Stanford HAI. (2024). *AI Index Report 2024.* https://hai.stanford.edu/ai-index/2024-ai-index-report  
12. Pew Research Center. (2023). Public Attitudes Toward AI in the United States.  
13. Pew Research Center. (2024). How AI Experts and the Public View AI.  
14. Brookings Institution. (2024). AI perceptions, concerns, and use patterns in the American public.  
15. KPMG Australia & University of Melbourne. (2024). *Global AI Trust and Adoption Index.*  
16. Cybernews Research Group. (2024). Shadow AI in the workplace: Employee use of unapproved AI tools.  
17. Stanford University. (2024). AI Incidents and Harms Database: 10-Year Trend Analysis.  
18. PwC. (2025). 2025 Responsible AI and Emerging Technology Survey.  
19. Freeman, J., Kim, S., Patel, R. (2025). Governance readiness and adoption barriers for AI in clinical environments. *Journal of Medical Systems, 49*(2), 55.  
20. OECD. (2025). *Governing with AI: Implementation Capacity, Challenges, and Public Sector Applications.*  
21. UN DESA. (2024). *E-Government Survey 2024: AI Readiness and Digital Public Infrastructure.*  
22. Office of Management and Budget. (2024). Advancing Responsible AI in Government.  
23. IDC & Microsoft. (2024). Responsible AI in enterprise: Adoption, maturity, and financial risk assessment.  
24. UK DSIT. (2024). Evaluating political persuasion risks of large language models. AI Safety Institute Report.  
25. United Nations. (2023). *Governing AI for Humanity: Report of the UN Secretary-General’s Advisory Body on AI.*  
26. Nair, V. (2024). Barriers to safe AI deployment in healthcare. *BMC Health Services Research, 24*, 773.  
27. Bouderhem, S. (2024). Equity, interoperability, and trust in AI-enabled healthcare. *Health Policy and Technology, 13*(2), 100821.  
28. Hassan, R. (2024). AI governance in healthcare: Autonomy, trust, and system-level challenges. *International Journal of Medical Informatics, 189*, 105303.  
